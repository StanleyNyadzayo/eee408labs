{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC63n1uPL4hKE773a/9mn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanleyNyadzayo/eee408labs/blob/MLCP-1-Study-Supervised-vs-Unsupervised-learning-purpose-problem-types/lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing"
      ],
      "metadata": {
        "id": "UQ7zNsZHFTCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_pH8qC_BBZDX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('nlp').getOrCreate()\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer and RegexTokenizer are Classes using for splitting text into individual words or tokens. Tokenizer uses whitespace and RegexTokenizer uses regular expressions to define how the text should be split. Both are key for the NLP pipeline, preparing data for further analysis or model trainig"
      ],
      "metadata": {
        "id": "iiNlz2BgG5Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**col** references a column in a DataFrame.\n",
        "**udf** means a user defined function, making it possible for a user to define functions in python, scala or java. it can work when hanlding calculations that are not supported by the built in functions of PySpark.\n",
        "**IntergerType** is a data type in PySpark SQL representing 32 bit integer. it is useful when working with datat types that require specifying expected data type of a column\n"
      ],
      "metadata": {
        "id": "QXIQD3CXMzO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrame for NLP"
      ],
      "metadata": {
        "id": "bBDBARJcNw_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('nlp').getOrCreate()\n",
        "testSents = spark.createDataFrame([\n",
        "    (0, 'Eight months into the PhD!'),\n",
        "    (1, 'AI 2 Module is almost complete and Christmas is near'),\n",
        "    (2, 'Lets see what these tokens are all about'),\n",
        "    (3, 'The weather is doing alright!!!')\n",
        "],['id','sentence']) # defines the column names for the DataFrame\n",
        "testSents.show(truncate=False) # show() truncates the sentences so setting it to false will display full sentences\n",
        "testSents.printSchema() #to show the data types"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JptAkIDgO0F0",
        "outputId": "9b939beb-6fa6-43f4-dd8b-e1a9e648dce4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------------------------------+\n",
            "|id |sentence                                            |\n",
            "+---+----------------------------------------------------+\n",
            "|0  |Eight months into the PhD!                          |\n",
            "|1  |AI 2 Module is almost complete and Christmas is near|\n",
            "|2  |Lets see what these tokens are all about            |\n",
            "|3  |The weather is doing alright!!!                     |\n",
            "+---+----------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- sentence: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization"
      ],
      "metadata": {
        "id": "yIrMRJLjSoKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = Tokenizer(inputCol='sentence', outputCol='words')\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "tokenized = tokens.transform(testSents)\n",
        "tokenized.withColumn('tokens', countTokens(col('words'))).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrzMSyrQRuXC",
        "outputId": "68e33a32-93bc-4576-d944-6c96aaf69577"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------------------------------+---------------------------------------------------------------+------+\n",
            "|id |sentence                                            |words                                                          |tokens|\n",
            "+---+----------------------------------------------------+---------------------------------------------------------------+------+\n",
            "|0  |Eight months into the PhD!                          |[eight, months, into, the, phd!]                               |5     |\n",
            "|1  |AI 2 Module is almost complete and Christmas is near|[ai, 2, module, is, almost, complete, and, christmas, is, near]|10    |\n",
            "|2  |Lets see what these tokens are all about            |[lets, see, what, these, tokens, are, all, about]              |8     |\n",
            "|3  |The weather is doing alright!!!                     |[the, weather, is, doing, alright!!!]                          |5     |\n",
            "+---+----------------------------------------------------+---------------------------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexTokenizer allows more advanced tokenization based on regular expression\n",
        "(regex) matching. By default, the parameter “pattern” (regex, default: \"\\\\s+\") is used as\n",
        "delimiters to split the input text on spaces.**"
      ],
      "metadata": {
        "id": "gftXfFsFU_Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regexTokens2 = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')\n",
        "regexTokenized = regexTokens2.transform(testSents)\n",
        "regexTokenized.select(col(\"sentence\"), col(\"words\")).withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukNWIPGlS0gx",
        "outputId": "1dba6b73-d370-44cd-90c6-82e5daf2ec3b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------+---------------------------------------------------------------+------+\n",
            "|sentence                                            |words                                                          |tokens|\n",
            "+----------------------------------------------------+---------------------------------------------------------------+------+\n",
            "|Eight months into the PhD!                          |[eight, months, into, the, phd]                                |5     |\n",
            "|AI 2 Module is almost complete and Christmas is near|[ai, 2, module, is, almost, complete, and, christmas, is, near]|10    |\n",
            "|Lets see what these tokens are all about            |[lets, see, what, these, tokens, are, all, about]              |8     |\n",
            "|The weather is doing alright!!!                     |[the, weather, is, doing, alright]                             |5     |\n",
            "+----------------------------------------------------+---------------------------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is a feature vectorization method widely used in text mining to rflect the importance of a term to a document in the corpus."
      ],
      "metadata": {
        "id": "qREZf9Z5Wsp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(tokenized)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "rescaledData.select(\"id\", \"features\").show(truncate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4bFKavQV39H",
        "outputId": "b850a767-390a-4cc9-a266-8fc0bee82878"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|            features|\n",
            "+---+--------------------+\n",
            "|  0|(20,[0,2,6,11,17]...|\n",
            "|  1|(20,[6,8,9,11,12,...|\n",
            "|  2|(20,[3,6,11,12,14...|\n",
            "|  3|(20,[8,9,13,15,17...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StopWordsRemover:**\n",
        "Stop words are words which should be excluded from the input, typically because the\n",
        "words appear frequently and don’t carry as much meaning."
      ],
      "metadata": {
        "id": "Qg1XvwHEiuP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "remover.transform(tokenized).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDZK9mM4XdIc",
        "outputId": "8fbf6bce-c8f9-4882-ea1f-439f7ed7d4ca"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------------------------------+---------------------------------------------------------------+--------------------------------------------------+\n",
            "|id |sentence                                            |words                                                          |filtered                                          |\n",
            "+---+----------------------------------------------------+---------------------------------------------------------------+--------------------------------------------------+\n",
            "|0  |Eight months into the PhD!                          |[eight, months, into, the, phd!]                               |[eight, months, phd!]                             |\n",
            "|1  |AI 2 Module is almost complete and Christmas is near|[ai, 2, module, is, almost, complete, and, christmas, is, near]|[ai, 2, module, almost, complete, christmas, near]|\n",
            "|2  |Lets see what these tokens are all about            |[lets, see, what, these, tokens, are, all, about]              |[lets, see, tokens]                               |\n",
            "|3  |The weather is doing alright!!!                     |[the, weather, is, doing, alright!!!]                          |[weather, alright!!!]                             |\n",
            "+---+----------------------------------------------------+---------------------------------------------------------------+--------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pkS5ibjSi5Qk"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}